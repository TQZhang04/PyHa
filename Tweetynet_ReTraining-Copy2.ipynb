{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e9ee970",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from PyHa.statistics import *\n",
    "from PyHa.IsoAutio import *\n",
    "from PyHa.visualizations import *\n",
    "from PyHa.annotation_post_processing import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0361ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./mixed_bird/025_Mixed_Bird_Outputs_Train/\"\n",
    "manual_csv = \"./mixed_bird_manual.csv\"\n",
    "device_name = \"cpu\"#\"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37ef9052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyHa.tweetynet_package.tweetynet.Load_data_functions import compute_features, predictions_to_kaleidoscope\n",
    "from PyHa.microfaune_package.microfaune import audio\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795746a7",
   "metadata": {},
   "source": [
    "# PREP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dad9b236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.kaledoscope_dataset at 0x236f50f7988>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "class kaledoscope_dataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file,filepath, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.manual_df = pd.read_csv(csv_file)\n",
    "        self.manual_df[\"FOLDER\"] = self.manual_df[\"IN FILE\"].apply(lambda x: filepath)\n",
    "        self.audio_files = self.manual_df[[\"FOLDER\",\"IN FILE\"]].drop_duplicates().reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.manual_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #TODO: FIX MAGIC NUMBERS\n",
    "        normalized_sample_rate=44100\n",
    "\n",
    "        audio_df = self.audio_files.iloc[idx]\n",
    "        \n",
    "        audio_dif = audio_df[\"FOLDER\"]\n",
    "        audio_file = audio_df[\"IN FILE\"]\n",
    "        try:\n",
    "            SAMPLE_RATE, SIGNAL = audio.load_wav(audio_dif + audio_file)\n",
    "        except BaseException as e:\n",
    "            #print(\"Failed to load\", audio_dif + audio_file)\n",
    "            #print(e)\n",
    "            return (-1,-1)\n",
    "        \n",
    "        try:\n",
    "            if SAMPLE_RATE != normalized_sample_rate:\n",
    "                rate_ratio = normalized_sample_rate / SAMPLE_RATE\n",
    "                SIGNAL = scipy_signal.resample(\n",
    "                    SIGNAL, int(len(SIGNAL) * rate_ratio))\n",
    "                SAMPLE_RATE = normalized_sample_rate\n",
    "        except:\n",
    "            print(\"Failed to Downsample\" + audio_file)\n",
    "            return (-1,-1)\n",
    "            \n",
    "        if len(SIGNAL.shape) == 2:\n",
    "            SIGNAL = SIGNAL.sum(axis=1) / 2\n",
    "        # detection\n",
    "        try:\n",
    "            tweetynet_features = compute_features([SIGNAL])\n",
    "        except:\n",
    "            print(\"Failed to compute features\" + audio_file)\n",
    "            return (-1,-1)\n",
    "        \n",
    "\n",
    "            \n",
    "        label_df = self.manual_df[(self.manual_df[\"FOLDER\"] == audio_df[\"FOLDER\"]) & (self.manual_df[\"IN FILE\"] == audio_df[\"IN FILE\"])]\n",
    "        return (tweetynet_features, label_df, audio_dif, audio_file, SIGNAL, SAMPLE_RATE)\n",
    "    \n",
    "trainloader = kaledoscope_dataset(manual_csv, path)\n",
    "trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e887849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init detector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae16845",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b7ecf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: DEFINE TEST LOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2b7441d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def IOU_Loss(automated_df, manual_df):\n",
    "    IoUMatrix = torch.tensor(clip_IoU(automated_df, manual_df));\n",
    "    return torch.mean(IoUMatrix)\n",
    "\n",
    "def convert_label_to_local_score(manual_df, size_of_local_sorce, start_time = 0, duration_of_clip = -1):\n",
    "    if (manual_df.empty):\n",
    "        return torch.tensor(np.ones(size_of_local_sorce)).float()\n",
    "    if (duration_of_clip == -1):\n",
    "        duration_of_clip = manual_df.iloc[0][\"CLIP LENGTH\"]\n",
    "    seconds_per_index = duration_of_clip/size_of_local_sorce\n",
    "    local_score = np.ones(size_of_local_sorce)\n",
    "    for i in range(size_of_local_sorce):\n",
    "        current_seconds = i * seconds_per_index + start_time\n",
    "        annotations_at_time = manual_df[(manual_df[\"OFFSET\"] <= current_seconds) & (manual_df[\"OFFSET\"] +manual_df[\"DURATION\"] >=  current_seconds)]\n",
    "        if (not annotations_at_time.empty):\n",
    "            local_score[i] = 0\n",
    "    \n",
    "    return torch.tensor(local_score).float()\n",
    "\n",
    "\n",
    "\n",
    "manual_df  = pd.read_csv(\"ScreamingPiha_Manual_Labels.csv\")\n",
    "convert_label_to_local_score(manual_df[manual_df[\"IN FILE\"] == \"ScreamingPiha5.wav\"], 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c746c5c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed  1  files\n",
      "Processed  2  files\n",
      "Processed  3  files\n",
      "Processed  4  files\n",
      "Processed  5  files\n",
      "Processed  6  files\n",
      "Processed  7  files\n",
      "Processed  8  files\n",
      "Processed  9  files\n",
      "Processed  10  files\n",
      "Processed  11  files\n",
      "Processed  12  files\n",
      "Processed  13  files\n",
      "Processed  14  files\n",
      "Processed  15  files\n",
      "Processed  16  files\n",
      "Processed  17  files\n",
      "Processed  18  files\n",
      "Processed  19  files\n",
      "Processed  20  files\n",
      "Processed  21  files\n",
      "Processed  22  files\n",
      "Processed  23  files\n",
      "Processed  24  files\n",
      "Processed  25  files\n",
      "Processed  26  files\n",
      "Processed  27  files\n",
      "Processed  28  files\n",
      "Processed  29  files\n",
      "Processed  30  files\n",
      "Processed  31  files\n",
      "Processed  32  files\n",
      "Processed  33  files\n",
      "Processed  34  files\n",
      "Processed  35  files\n",
      "Processed  36  files\n",
      "Processed  37  files\n",
      "Processed  38  files\n",
      "Processed  39  files\n",
      "Processed  40  files\n",
      "Processed  41  files\n",
      "Processed  42  files\n",
      "Processed  43  files\n",
      "Processed  44  files\n",
      "Processed  45  files\n",
      "Processed  46  files\n",
      "Processed  47  files\n",
      "Processed  48  files\n",
      "Processed  49  files\n",
      "Processed  50  files\n",
      "Processed  51  files\n",
      "Processed  52  files\n",
      "Processed  53  files\n",
      "Processed  54  files\n",
      "Processed  55  files\n",
      "Processed  56  files\n",
      "Processed  57  files\n",
      "Processed  58  files\n",
      "Processed  59  files\n",
      "Processed  60  files\n",
      "Processed  61  files\n",
      "Processed  62  files\n",
      "Processed  63  files\n",
      "Processed  64  files\n",
      "Processed  65  files\n",
      "Processed  66  files\n",
      "Processed  67  files\n",
      "Processed  68  files\n",
      "Processed  69  files\n",
      "Processed  70  files\n",
      "Processed  71  files\n",
      "Processed  72  files\n",
      "Processed  73  files\n",
      "Processed  74  files\n",
      "Processed  75  files\n",
      "Processed  76  files\n",
      "Processed  77  files\n",
      "Processed  78  files\n",
      "Processed  79  files\n",
      "Processed  80  files\n",
      "Processed  81  files\n",
      "Processed  82  files\n",
      "Processed  83  files\n",
      "Processed  84  files\n",
      "Processed  85  files\n",
      "Processed  86  files\n",
      "Processed  87  files\n",
      "Processed  88  files\n",
      "Processed  89  files\n",
      "Processed  90  files\n",
      "Processed  91  files\n",
      "Processed  92  files\n",
      "Processed  93  files\n",
      "Processed  94  files\n",
      "Processed  95  files\n",
      "Processed  96  files\n",
      "Processed  97  files\n",
      "Processed  98  files\n",
      "Processed  99  files\n",
      "Processed  100  files\n",
      "Processed  101  files\n",
      "Processed  102  files\n",
      "Processed  103  files\n",
      "Processed  104  files\n",
      "Processed  105  files\n",
      "Processed  106  files\n",
      "Processed  107  files\n",
      "Processed  108  files\n",
      "Processed  109  files\n",
      "Processed  110  files\n",
      "Processed  111  files\n",
      "Processed  112  files\n",
      "Processed  113  files\n",
      "Processed  114  files\n",
      "Processed  115  files\n",
      "Processed  116  files\n",
      "Processed  117  files\n",
      "Processed  118  files\n",
      "Processed  119  files\n",
      "Processed  120  files\n",
      "Processed  121  files\n",
      "Processed  122  files\n",
      "Processed  123  files\n",
      "Processed  124  files\n",
      "Processed  125  files\n",
      "Processed  126  files\n",
      "Processed  127  files\n",
      "Processed  128  files\n",
      "Processed  129  files\n",
      "Processed  130  files\n",
      "Processed  131  files\n",
      "Processed  132  files\n",
      "Processed  133  files\n",
      "Processed  134  files\n",
      "Processed  135  files\n",
      "Processed  136  files\n",
      "Processed  137  files\n",
      "Processed  138  files\n",
      "Processed  139  files\n",
      "Processed  140  files\n",
      "Processed  141  files\n",
      "Processed  142  files\n",
      "Processed  143  files\n",
      "Processed  144  files\n",
      "Processed  145  files\n",
      "Processed  146  files\n",
      "Processed  147  files\n",
      "Processed  148  files\n",
      "Processed  149  files\n",
      "Processed  150  files\n",
      "Processed  151  files\n",
      "Processed  152  files\n",
      "Processed  153  files\n",
      "Processed  154  files\n",
      "Processed  155  files\n",
      "Processed  156  files\n",
      "Processed  157  files\n",
      "Processed  158  files\n",
      "Processed  159  files\n",
      "Processed  160  files\n",
      "Processed  161  files\n",
      "Processed  162  files\n",
      "Processed  163  files\n",
      "Processed  164  files\n",
      "Processed  165  files\n",
      "Processed  166  files\n",
      "Processed  167  files\n",
      "Processed  168  files\n",
      "Processed  169  files\n",
      "Processed  170  files\n",
      "Processed  171  files\n",
      "Processed  172  files\n",
      "Processed  173  files\n",
      "Processed  174  files\n",
      "Processed  175  files\n",
      "Processed  176  files\n",
      "Processed  177  files\n",
      "Processed  178  files\n",
      "Processed  179  files\n",
      "Processed  180  files\n",
      "Processed  181  files\n",
      "Processed  182  files\n",
      "Processed  183  files\n",
      "Processed  184  files\n",
      "Processed  185  files\n",
      "Processed  186  files\n",
      "Processed  187  files\n",
      "Processed  188  files\n",
      "Processed  189  files\n",
      "Processed  190  files\n",
      "Processed  191  files\n",
      "Processed  192  files\n",
      "Processed  193  files\n",
      "Processed  194  files\n",
      "Processed  195  files\n",
      "Processed  196  files\n",
      "Processed  197  files\n",
      "Processed  198  files\n",
      "Processed  199  files\n",
      "Processed  200  files\n",
      "Processed  201  files\n",
      "Processed  202  files\n",
      "Processed  203  files\n",
      "Processed  204  files\n",
      "Processed  205  files\n",
      "Processed  206  files\n",
      "Processed  207  files\n",
      "Processed  208  files\n",
      "Processed  209  files\n",
      "Processed  210  files\n",
      "Processed  211  files\n",
      "Processed  212  files\n",
      "Processed  213  files\n",
      "Processed  214  files\n",
      "Processed  215  files\n",
      "Processed  216  files\n",
      "Processed  217  files\n",
      "Processed  218  files\n",
      "Processed  219  files\n",
      "Processed  220  files\n",
      "Processed  221  files\n",
      "Processed  222  files\n",
      "Processed  223  files\n",
      "Processed  224  files\n",
      "Processed  225  files\n",
      "Processed  226  files\n",
      "Processed  227  files\n",
      "Processed  228  files\n",
      "Processed  229  files\n",
      "Processed  230  files\n",
      "Processed  231  files\n",
      "Processed  232  files\n",
      "Processed  233  files\n",
      "Processed  234  files\n",
      "Processed  235  files\n",
      "Processed  236  files\n",
      "Processed  237  files\n",
      "Processed  238  files\n",
      "Processed  239  files\n",
      "Processed  240  files\n",
      "Processed  241  files\n",
      "Processed  242  files\n",
      "Processed  243  files\n",
      "Processed  244  files\n",
      "Processed  245  files\n",
      "Processed  246  files\n",
      "Processed  247  files\n",
      "Processed  248  files\n",
      "Processed  249  files\n",
      "Processed  250  files\n",
      "Processed  251  files\n",
      "Processed  252  files\n",
      "Processed  253  files\n",
      "Processed  254  files\n",
      "Processed  255  files\n",
      "Processed  256  files\n",
      "Processed  257  files\n",
      "Processed  258  files\n",
      "Processed  259  files\n",
      "Processed  260  files\n",
      "Processed  261  files\n",
      "Processed  262  files\n",
      "Processed  263  files\n",
      "Processed  264  files\n",
      "Processed  265  files\n",
      "Processed  266  files\n",
      "Processed  267  files\n",
      "Processed  268  files\n",
      "Processed  269  files\n",
      "Processed  270  files\n",
      "Processed  271  files\n",
      "Processed  272  files\n",
      "Processed  273  files\n",
      "Processed  274  files\n",
      "Processed  275  files\n",
      "Processed  276  files\n",
      "Processed  277  files\n",
      "Processed  278  files\n",
      "Processed  279  files\n",
      "Processed  280  files\n",
      "Processed  281  files\n",
      "Processed  282  files\n",
      "Processed  283  files\n",
      "Processed  284  files\n",
      "Processed  285  files\n",
      "Processed  286  files\n",
      "Processed  287  files\n",
      "Processed  288  files\n",
      "Processed  289  files\n",
      "Processed  290  files\n",
      "Processed  291  files\n",
      "Processed  292  files\n",
      "Processed  293  files\n",
      "Processed  294  files\n",
      "Processed  295  files\n",
      "Processed  296  files\n",
      "Processed  297  files\n",
      "Processed  298  files\n",
      "Processed  299  files\n",
      "Processed  300  files\n",
      "Processed  301  files\n",
      "Processed  302  files\n",
      "Processed  303  files\n",
      "Processed  304  files\n",
      "Processed  305  files\n",
      "Processed  306  files\n",
      "Processed  307  files\n",
      "Processed  308  files\n",
      "Processed  309  files\n",
      "Processed  310  files\n",
      "Processed  311  files\n",
      "Processed  312  files\n",
      "Processed  313  files\n",
      "Processed  314  files\n",
      "Processed  315  files\n",
      "Processed  316  files\n",
      "Processed  317  files\n",
      "Processed  318  files\n",
      "Processed  319  files\n",
      "Processed  320  files\n",
      "Processed  321  files\n",
      "Processed  322  files\n",
      "Processed  323  files\n",
      "Processed  324  files\n",
      "Processed  325  files\n",
      "Processed  326  files\n",
      "Processed  327  files\n",
      "Processed  328  files\n",
      "Processed  329  files\n",
      "Processed  330  files\n",
      "Processed  331  files\n",
      "Processed  332  files\n",
      "Processed  333  files\n",
      "Processed  334  files\n",
      "Processed  335  files\n",
      "Processed  336  files\n",
      "Processed  337  files\n",
      "Processed  338  files\n",
      "Processed  339  files\n",
      "Processed  340  files\n",
      "Processed  341  files\n",
      "Processed  342  files\n",
      "Processed  343  files\n",
      "Processed  344  files\n",
      "Processed  345  files\n",
      "Processed  346  files\n",
      "Processed  347  files\n",
      "Processed  348  files\n",
      "Processed  349  files\n",
      "Processed  350  files\n",
      "Processed  351  files\n",
      "Processed  352  files\n",
      "Processed  353  files\n",
      "Processed  354  files\n",
      "Processed  355  files\n",
      "Processed  356  files\n",
      "Processed  357  files\n",
      "Processed  358  files\n",
      "Processed  359  files\n",
      "Processed  360  files\n",
      "Processed  361  files\n",
      "Processed  362  files\n",
      "Processed  363  files\n",
      "Processed  364  files\n",
      "Processed  365  files\n",
      "Processed  366  files\n",
      "Processed  367  files\n",
      "Processed  368  files\n",
      "Processed  369  files\n",
      "Processed  370  files\n",
      "Processed  371  files\n",
      "Processed  372  files\n",
      "Processed  373  files\n",
      "Processed  374  files\n",
      "Processed  375  files\n",
      "Processed  376  files\n",
      "Processed  377  files\n",
      "Processed  378  files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed  379  files\n",
      "Processed  380  files\n",
      "Processed  381  files\n",
      "Processed  382  files\n",
      "Processed  383  files\n",
      "Processed  384  files\n",
      "Processed  385  files\n",
      "Processed  386  files\n",
      "Processed  387  files\n",
      "Processed  388  files\n",
      "Processed  389  files\n",
      "Processed  390  files\n",
      "Processed  391  files\n",
      "Processed  392  files\n",
      "Processed  393  files\n",
      "Processed  394  files\n",
      "Processed  395  files\n",
      "Processed  396  files\n",
      "Processed  397  files\n",
      "Processed  398  files\n",
      "Processed  399  files\n",
      "Processed  400  files\n",
      "Processed  401  files\n",
      "Processed  402  files\n",
      "Processed  403  files\n",
      "Processed  404  files\n",
      "Processed  405  files\n",
      "Processed  406  files\n",
      "Processed  407  files\n",
      "Processed  408  files\n",
      "Processed  409  files\n",
      "Processed  410  files\n",
      "Processed  411  files\n",
      "Processed  412  files\n",
      "Processed  413  files\n",
      "Processed  414  files\n",
      "Processed  415  files\n",
      "Processed  416  files\n",
      "Processed  417  files\n",
      "Processed  418  files\n",
      "Processed  419  files\n",
      "Processed  420  files\n",
      "Processed  421  files\n",
      "Processed  422  files\n",
      "Processed  423  files\n",
      "Processed  424  files\n",
      "Processed  425  files\n",
      "Processed  426  files\n",
      "Processed  427  files\n",
      "Processed  428  files\n",
      "Processed  429  files\n",
      "Processed  430  files\n",
      "Processed  431  files\n",
      "Processed  432  files\n",
      "Processed  433  files\n",
      "Processed  434  files\n",
      "Processed  435  files\n",
      "Processed  436  files\n",
      "Processed  437  files\n",
      "Processed  438  files\n",
      "Processed  439  files\n",
      "Processed  440  files\n",
      "Processed  441  files\n",
      "Processed  442  files\n",
      "Processed  443  files\n",
      "Processed  444  files\n",
      "Processed  445  files\n",
      "Processed  446  files\n",
      "Processed  447  files\n",
      "Processed  448  files\n",
      "Processed  449  files\n",
      "Processed  450  files\n",
      "Processed  451  files\n",
      "Processed  452  files\n",
      "Processed  453  files\n",
      "Processed  454  files\n",
      "Processed  455  files\n",
      "Processed  456  files\n",
      "Processed  457  files\n",
      "Processed  458  files\n",
      "Processed  459  files\n",
      "Processed  460  files\n",
      "Processed  461  files\n",
      "Processed  462  files\n",
      "Processed  463  files\n",
      "Processed  464  files\n",
      "Processed  465  files\n",
      "Processed  466  files\n",
      "Processed  467  files\n",
      "Processed  468  files\n",
      "Processed  469  files\n",
      "Processed  470  files\n",
      "Processed  471  files\n",
      "Processed  472  files\n",
      "Processed  473  files\n",
      "Processed  474  files\n",
      "Processed  475  files\n",
      "Processed  476  files\n",
      "Processed  477  files\n",
      "Processed  478  files\n",
      "Processed  479  files\n",
      "Processed  480  files\n",
      "Processed  481  files\n",
      "Processed  482  files\n",
      "Processed  483  files\n",
      "Processed  484  files\n",
      "Processed  485  files\n",
      "Processed  486  files\n",
      "Processed  487  files\n",
      "Processed  488  files\n",
      "Processed  489  files\n",
      "Processed  490  files\n",
      "Processed  491  files\n",
      "Processed  492  files\n",
      "Processed  493  files\n",
      "Processed  494  files\n",
      "Processed  495  files\n",
      "Processed  496  files\n",
      "Processed  497  files\n",
      "Processed  498  files\n",
      "Processed  499  files\n",
      "Processed  500  files\n",
      "Processed  501  files\n",
      "Processed  502  files\n",
      "Processed  503  files\n",
      "Processed  504  files\n",
      "Processed  505  files\n",
      "Processed  506  files\n",
      "Processed  507  files\n",
      "Processed  508  files\n",
      "Processed  509  files\n",
      "Processed  510  files\n",
      "Processed  511  files\n",
      "Processed  512  files\n",
      "Processed  513  files\n",
      "Processed  514  files\n",
      "Processed  515  files\n",
      "Processed  516  files\n",
      "Processed  517  files\n",
      "Processed  518  files\n",
      "Processed  519  files\n",
      "Processed  520  files\n",
      "Processed  521  files\n",
      "Processed  522  files\n",
      "Processed  523  files\n",
      "Processed  524  files\n",
      "Processed  525  files\n",
      "Processed  526  files\n",
      "Processed  527  files\n",
      "Processed  528  files\n",
      "Processed  529  files\n",
      "Processed  530  files\n",
      "Processed  531  files\n",
      "Processed  532  files\n",
      "Processed  533  files\n",
      "Processed  534  files\n",
      "Processed  535  files\n",
      "Processed  536  files\n",
      "Processed  537  files\n",
      "Processed  538  files\n",
      "Processed  539  files\n",
      "Processed  540  files\n",
      "Processed  541  files\n",
      "Processed  542  files\n",
      "Processed  543  files\n",
      "Processed  544  files\n",
      "Processed  545  files\n",
      "Processed  546  files\n",
      "Processed  547  files\n",
      "Processed  548  files\n",
      "Processed  549  files\n",
      "Processed  550  files\n",
      "Processed  551  files\n",
      "Processed  552  files\n",
      "Processed  553  files\n",
      "Processed  554  files\n",
      "Processed  555  files\n",
      "Processed  556  files\n",
      "Processed  557  files\n",
      "Processed  558  files\n",
      "Processed  559  files\n",
      "Processed  560  files\n",
      "Processed  561  files\n",
      "Processed  562  files\n",
      "Processed  563  files\n",
      "Processed  564  files\n",
      "Processed  565  files\n",
      "Processed  566  files\n",
      "Processed  567  files\n",
      "Processed  568  files\n",
      "Processed  569  files\n",
      "Processed  570  files\n",
      "Processed  571  files\n",
      "Processed  572  files\n",
      "Processed  573  files\n",
      "Processed  574  files\n",
      "Processed  575  files\n",
      "Processed  576  files\n",
      "Processed  577  files\n",
      "Processed  578  files\n",
      "Processed  579  files\n",
      "Processed  580  files\n",
      "Processed  581  files\n",
      "Processed  582  files\n",
      "Processed  583  files\n",
      "Processed  584  files\n",
      "Processed  585  files\n",
      "Processed  586  files\n",
      "Processed  587  files\n",
      "Processed  588  files\n",
      "Processed  589  files\n",
      "Processed  590  files\n",
      "Processed  591  files\n",
      "Processed  592  files\n",
      "Processed  593  files\n",
      "Processed  594  files\n",
      "Processed  595  files\n",
      "Processed  596  files\n",
      "Processed  597  files\n",
      "Processed  598  files\n",
      "Processed  599  files\n",
      "Processed  600  files\n",
      "Processed  601  files\n",
      "Processed  602  files\n",
      "Processed  603  files\n",
      "Processed  604  files\n",
      "1:  tensor(0.9581)\n",
      "0:  tensor(0.0419)\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-d8b6fd0b4cd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"1: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mones\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtotal_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"0: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtotal_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'[{epoch + 1, batch_count}] loss: {batch_loss / batch_count:.3f}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m     \u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[0mbatch_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "device = torch.device(device_name)\n",
    "net_wrapper = TweetyNetModel(2, (1, 86, 86), 86, device)\n",
    "net_wrapper.model = net_wrapper.model.to(device)\n",
    "\n",
    "\n",
    "net = net_wrapper.model\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = True\n",
    "optimizer = optim.Adam(net_wrapper.model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    batch_loss = 0.0\n",
    "    mini_batch_count = 0\n",
    "    batch_count = 0\n",
    "    file_count = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        try:\n",
    "            inputs, labels_df, audio_dif, audio_file, SIGNAL, SAMPLE_RATE = data\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        net = net_wrapper.model\n",
    "        for param in net.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        batch_size=1\n",
    "        window_size=2\n",
    "        test_data_loader = DataLoader(inputs, batch_size=batch_size)\n",
    "        predictions = pd.DataFrame()\n",
    "        #net_wrapper.model.eval()\n",
    "        local_score = torch.tensor([])\n",
    "        tmp_local_score = []\n",
    "        dataiter = iter(test_data_loader)\n",
    "        _, label, uid = dataiter.next()\n",
    "        time_bin = float(window_size)/label.shape[1]\n",
    "        st_time = np.array([time_bin*n for n in range(label.shape[1])])\n",
    "        count = []\n",
    "        \n",
    "        total_count = 0\n",
    "        ones = 0\n",
    "        zeros = 0\n",
    "\n",
    "        \n",
    "        for i, data in enumerate(test_data_loader):\n",
    "                \n",
    "            #Run Model\n",
    "            sub_inputs, sub_labels, uids = data\n",
    "            sub_inputs = sub_inputs.clone().detach().requires_grad_(True)\n",
    "            sub_inputs, sub_labels = sub_inputs.to(device), sub_labels.to(device)\n",
    "            output = net_wrapper.model.to(device)(sub_inputs, sub_inputs.shape[0], sub_inputs.shape[0])\n",
    "\n",
    "            #Get labels for window\n",
    "            bins = st_time + (int(uids[0].split(\"_\")[0])*window_size)\n",
    "            duration_of_window = bins[-1] -bins[0]\n",
    "            #print(convert_label_to_local_score(labels_df, len([x for x in output[0, 1, :]])))\n",
    "            label_for_output = convert_label_to_local_score(labels_df, 86, bins[0],duration_of_window)\n",
    "            \n",
    "            \n",
    "            total_count += 86\n",
    "            ones += sum(label_for_output)\n",
    "            zeros += 86 - sum(label_for_output)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #label_for_output = convert_label_to_local_score(labels_df, len([x for x in output[0, 1, :]]), bins[0],duration_of_window)\n",
    "            \n",
    "            #Set up output for correct batch * class matrix\n",
    "            #output_for_loss = torch.transpose(output.clone()[0].clone(), 0, 1).clone().float()\n",
    "            \n",
    "            #Compute Loss\n",
    "            #output = output_for_loss\n",
    "            #print(output)\n",
    "            \n",
    "            #TODO REMOVE\n",
    "            #print(output)\n",
    "            #print(\"============\")\n",
    "            #m = nn.Softmax(dim=1)\n",
    "            #output = m(output)\n",
    "            #print(output)\n",
    "            #TODO REMOVE\n",
    "            \n",
    "            #label_array =label_for_output.to(device)\n",
    "            #label_array = label_array.long().to(device)\n",
    "            \n",
    "            #print(label_array, output)\n",
    "            #loss = criterion(output, label_array)\n",
    "            \n",
    "            #Backprop + Check for learning\n",
    "            #state_a = net_wrapper.model.state_dict().__str__()\n",
    "            #loss.backward()\n",
    "            #optimizer.step()\n",
    "            #state_b = net_wrapper.model.state_dict().__str__()\n",
    "            #if state_a == state_b:\n",
    "            #    print(\"Network not updating.\")\n",
    "\n",
    "\n",
    "            # print statistics\n",
    "            #running_loss += loss.item()\n",
    "            #batch_loss += loss.item()\n",
    "            #mini_batch_count += 1\n",
    "            #batch_count += 1\n",
    "            #print(\"=======================\")\n",
    "        \n",
    "        #print(f'[{epoch + 1, mini_batch_count}] loss: {running_loss / mini_batch_count:.3f}')\n",
    "        file_count += 1\n",
    "        #if(file_count > 5):\n",
    "        print(\"Processed \", file_count, \" files\")\n",
    "        \n",
    "    print(\"1: \", ones/total_count)\n",
    "    print(\"0: \", zeros/total_count)\n",
    "    print(f'[{epoch + 1, batch_count}] loss: {batch_loss / batch_count:.3f}')\n",
    "    batch_loss = 0.0\n",
    "    batch_count = 0\n",
    "    break\n",
    "\n",
    "print('Finished Training')\n",
    "torch.save(net.state_dict(), \"./test.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ec195b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ae002d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net_wrapper.model.eval()\n",
    "%reload_ext autoreload\n",
    "correct = 0\n",
    "total = 0\n",
    "#Use Pyha with new weights to test performance\n",
    "isolation_parameters = {\n",
    "    \"model\" : \"tweetynet\",\n",
    "     \"tweety_output\": True,\n",
    "    \"technique\" : \"steinberg\",\n",
    "     \"threshold_type\" : \"median\",\n",
    "     \"threshold_const\" : 2.0,\n",
    "     \"threshold_min\" : 0.0,\n",
    "     \"window_size\" : 2.0,\n",
    "     \"chunk_size\" : 5.0\n",
    "}\n",
    "automated_df = generate_automated_labels(path,isolation_parameters, weight_path=\"./test.h5\");\n",
    "display(automated_df)\n",
    "manual_df = pd.read_csv(\"mixed_bird_manual.csv\")\n",
    "statistics_df = automated_labeling_statistics(automated_df,manual_df,stats_type = \"general\");\n",
    "statistics_df\n",
    "global_dataset_statistics(statistics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d23eb98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
